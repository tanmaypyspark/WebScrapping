{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Configure the logger (do this once at the beginning of your script)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a GET Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    DRIVER = 30\n",
    "    SCROLL_PAUSE_TIME = 5 #seconds\n",
    "    MAX_SCROLLS = 1\n",
    "    CHROME_OPTIONS = \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    BASE_PATH = 'if any base path is given'\n",
    "    DRIVER_LOCATION = '444556454c4f504544206279203a2054414e414d59204d414e44414c'\n",
    "    \n",
    "    def __init__(self,url, element_selector, ts_selector, author_selector):\n",
    "        self.url = url\n",
    "        self.element_selector = element_selector # This is the comment selector\n",
    "        self.timeStamp_selector = ts_selector # This is the timestamp selector\n",
    "        self.cmd_selector = author_selector # This is the author selector\n",
    "        self.driver_loc = bytes.fromhex(Scraper.DRIVER_LOCATION).decode('utf-8')\n",
    "        logger.info(self.driver_loc)\n",
    "        '''Start the Scraper'''\n",
    "        logger.info(\"Scraper has been started....\")\n",
    "                \n",
    "    def __convert_time_to_datetime(self, time_str):\n",
    "        \"\"\"Converts relative or absolute time string to \"DD-MM-YYYY HH:MM\" format.\"\"\"\n",
    "        try:\n",
    "            if \"mins\" in time_str or \"secs\" in time_str:\n",
    "                parts = time_str.split()\n",
    "                minutes = 0\n",
    "                seconds = 0\n",
    "\n",
    "                if \"mins\" in time_str:\n",
    "                    minutes = int(parts[0])\n",
    "                if \"secs\" in time_str:\n",
    "                    if \"mins\" in time_str:\n",
    "                        seconds = int(parts[2])\n",
    "                    else:\n",
    "                        seconds = int(parts[0])\n",
    "\n",
    "                now = datetime.datetime.now()\n",
    "                time_delta = datetime.timedelta(minutes=minutes, seconds=seconds)\n",
    "                past_time = now - time_delta\n",
    "                return past_time.strftime(\"%d-%m-%Y %H:%M\")\n",
    "\n",
    "            else:\n",
    "                # Absolute time (e.g., \"10:12 AM Feb 12th\" or \"7:29 AM Aug 28th\")\n",
    "                time_str = time_str.replace(\"th\", \"\").replace(\"st\",\"\").replace(\"nd\",\"\").replace(\"rd\",\"\") #remove suffixes\n",
    "                try:\n",
    "                    dt_object = datetime.datetime.strptime(time_str, \"%I:%M %p %b %d %Y\")\n",
    "                    return dt_object.strftime(\"%d-%m-%Y %H:%M\")\n",
    "                except ValueError:\n",
    "                    #if year is not present, add current year.\n",
    "                    now = datetime.datetime.now()\n",
    "                    dt_object = datetime.datetime.strptime(time_str + \" \" + str(now.year), \"%I:%M %p %b %d %Y\")\n",
    "                    #check if the parsed date is in the future. If so, subtract one year.\n",
    "                    if dt_object > now :\n",
    "                        dt_object = datetime.datetime.strptime(time_str + \" \" + str(now.year -1), \"%I:%M %p %b %d %Y\")\n",
    "\n",
    "                    return dt_object.strftime(\"%d-%m-%Y %H:%M\")\n",
    "\n",
    "        except ValueError:\n",
    "            logger.warning(f\"Invalid time string: {time_str}\")\n",
    "            return None  # Return None if the input string is invalid\n",
    "    \n",
    "    def __scrape_infinite_scroll(self):\n",
    "        \"\"\"Scrapes data from a page with infinite scrolling.\"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(Scraper.CHROME_OPTIONS)\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        logger.info(\"Fetch the data from the website....\")\n",
    "        try:\n",
    "            driver.get(self.url)\n",
    "\n",
    "            # Wait for initial content to load\n",
    "            WebDriverWait(driver, Scraper.DRIVER).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, '.postItem_text_paragraph__3XhZQ'))\n",
    "            )\n",
    "\n",
    "            scroll_count = 0\n",
    "            while scroll_count < Scraper.MAX_SCROLLS:\n",
    "                # Scroll to the bottom of the page\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                # Wait for new content to load\n",
    "                time.sleep(Scraper.SCROLL_PAUSE_TIME)\n",
    "\n",
    "                # Check if we've reached the end (optional, but helpful)\n",
    "                # You might need to adjust this check based on your website's behavior\n",
    "                try:\n",
    "                    # Example: If a specific element disappears when all content is loaded\n",
    "                    if not driver.find_elements(By.CSS_SELECTOR, \"/html\"):\n",
    "                        logger.info(\"Reached end of content.\")\n",
    "                        break\n",
    "                except:\n",
    "                    pass #if the check fails, just continue scrolling\n",
    "\n",
    "                scroll_count += 1\n",
    "\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            logger.info(\"The data fetched completed from the website....\")\n",
    "            return soup\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "    def get_the_data(self):\n",
    "        ''' Get the data from the website'''\n",
    "        return self.__scrape_infinite_scroll()\n",
    "    \n",
    "    def __find_Comments(self, stock_name):\n",
    "        ''' Find All the comments in the page'''\n",
    "        logger.info(\"Find all the comments in the page....\")\n",
    "        elements = self.__scrape_infinite_scroll()\n",
    "        comments = []\n",
    "        all_comments = []\n",
    "\n",
    "        for comment_element in elements.find_all(\"div\", class_= \".postItem_text_paragraph__3XhZQ\"):  # Replace class name\n",
    "            # Get the comment\n",
    "            comment_text = comment_element.get_text(strip=True)\n",
    "            #Get the timestamp\n",
    "            get_ts = comment_element.find_previous(\"div\", class_= self.timeStamp_selector) # Replace with your timestamp class\n",
    "            # Get the Author\n",
    "            author = comment_element.find_previous(\"div\", class_= self.cmd_selector).text.strip()\n",
    "            if get_ts:\n",
    "                timestamp_text = self.__convert_time_to_datetime(get_ts.find(\"div\").text.strip().replace(\"schedule\", \"\"))\n",
    "            else:\n",
    "                timestamp_text = \"Timestamp not found\"\n",
    "\n",
    "            all_comments.append({\"Stock\": stock_name, \"Comment\": comment_text,'Commented By':author, \"Timestamp\": timestamp_text})\n",
    "        \n",
    "        logger.info(\"Successfully grab the Data!!\")\n",
    "        \n",
    "        return all_comments\n",
    "    \n",
    "    def __Convert_to_DataFrame(self, stock_name):\n",
    "        ''' Convert the comments to a pandas dataframe'''\n",
    "        all_comments = self.__find_Comments(stock_name)\n",
    "        df = pd.DataFrame(all_comments)\n",
    "        df = df.sort_values(by='Timestamp', ascending=True)\n",
    "        return df\n",
    "    \n",
    "    def show_Comments(self, stock_name):\n",
    "        ''' Display all the comments in the page'''\n",
    "        data = self.__Convert_to_DataFrame(stock_name)\n",
    "        logger.info(\"************* The DATA *************\")\n",
    "        logger.info(data)\n",
    "    \n",
    "    def save_Comments(self, stock_name):\n",
    "        ''' Save the comments to a csv file'''\n",
    "        data = self.__Convert_to_DataFrame(stock_name)\n",
    "        data.to_csv(f\"comments_{stock_name}.csv\", index=False) #BASE_PATH + \"comments.csv\"\n",
    "        logger.info(f\"File has been generated successfully!!\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config_file(filename):\n",
    "    \"\"\"Reads a config file and returns a dictionary of stock names and URLs.\"\"\"\n",
    "    config_data = {}\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()  # Remove leading/trailing whitespace\n",
    "                if line:  # Skip empty lines\n",
    "                    try:\n",
    "                        stock_name, url = line.split(': ', 1)  # Split at the first ': '\n",
    "                        config_data[stock_name] = url\n",
    "                    except ValueError:\n",
    "                        print(f\"Warning: Invalid line in config file: {line}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Config file '{filename}' not found.\")\n",
    "    return config_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://mmb.moneycontrol.com/forum-topics/stocks/deepak-nitrite-4602.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 19:11:41,286 - INFO - DEVELOPED by : TANAMY MANDAL\n",
      "2025-03-19 19:11:41,287 - INFO - Scraper has been started....\n",
      "2025-03-19 19:11:42,915 - INFO - Fetch the data from the website....\n",
      "2025-03-19 19:12:08,829 - INFO - The data fetched completed from the website....\n"
     ]
    }
   ],
   "source": [
    "obj = Scraper(url, element_selector, ts_selector, author_selector)\n",
    "# obj.save_Comments(stock_name)\n",
    "# obj.show_Comments(\"Deepak Nitrite\")\n",
    "df = obj.get_the_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Static Parameters\n",
    "config_filename = 'config.conf'\n",
    "element_selector = \"postItem_text_paragraph__3XhZQ\"\n",
    "ts_selector = \"postItem_price1__3ojct\"\n",
    "author_selector = \"postItem_heading__2odZU\"\n",
    "# stock_data = read_config_file(config_filename)\n",
    "\n",
    "# if stock_data:\n",
    "#     for stock_name, url in stock_data.items():\n",
    "#         print(f\"Stock: {stock_name}, URL: {url}\")\n",
    "#         obj = Scraper(url, element_selector, ts_selector, author_selector)\n",
    "#         obj.save_Comments(stock_name)\n",
    "# else:\n",
    "#     print(\"No data read from the config file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tkinter as tk\n",
    "# from tkinter import ttk, scrolledtext, filedialog, messagebox\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from bs4 import BeautifulSoup\n",
    "# import time\n",
    "# import datetime\n",
    "# import logging\n",
    "\n",
    "# # Configure logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# class Scraper:\n",
    "#     DRIVER = 30\n",
    "#     SCROLL_PAUSE_TIME = 5  # seconds\n",
    "#     MAX_SCROLLS = 1\n",
    "#     CHROME_OPTIONS = \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "#     BASE_PATH = ''  # Base path for saving files\n",
    "#     DRIVER_LOCATION = '444556454c4f504544206279203a2054414e414d59204d414e44414c'\n",
    "\n",
    "#     def __init__(self, url, element_selector, ts_selector, author_selector):\n",
    "#         self.url = url\n",
    "#         self.element_selector = element_selector  # This is the comment selector\n",
    "#         self.timeStamp_selector = ts_selector  # This is the timestamp selector\n",
    "#         self.cmd_selector = author_selector  # This is the author selector\n",
    "#         self.driver_loc = bytes.fromhex(Scraper.DRIVER_LOCATION).decode('utf-8')\n",
    "#         logger.info(\"Scraper has been started....\")\n",
    "#         logger.info(self.driver_loc)\n",
    "\n",
    "#     def __convert_time_to_datetime(self, time_str):\n",
    "#         \"\"\"Converts relative or absolute time string to \"DD-MM-YYYY HH:MM\" format.\"\"\"\n",
    "#         try:\n",
    "#             if \"mins\" in time_str or \"secs\" in time_str:\n",
    "#                 parts = time_str.split()\n",
    "#                 minutes = 0\n",
    "#                 seconds = 0\n",
    "\n",
    "#                 if \"mins\" in time_str:\n",
    "#                     minutes = int(parts[0])\n",
    "#                 if \"secs\" in time_str:\n",
    "#                     if \"mins\" in time_str:\n",
    "#                         seconds = int(parts[2])\n",
    "#                     else:\n",
    "#                         seconds = int(parts[0])\n",
    "\n",
    "#                 now = datetime.datetime.now()\n",
    "#                 time_delta = datetime.timedelta(minutes=minutes, seconds=seconds)\n",
    "#                 past_time = now - time_delta\n",
    "#                 return past_time.strftime(\"%d-%m-%Y %H:%M\")\n",
    "\n",
    "#             else:\n",
    "#                 time_str = time_str.replace(\"th\", \"\").replace(\"st\", \"\").replace(\"nd\", \"\").replace(\"rd\", \"\")\n",
    "#                 try:\n",
    "#                     dt_object = datetime.datetime.strptime(time_str, \"%I:%M %p %b %d %Y\")\n",
    "#                     return dt_object.strftime(\"%d-%m-%Y %H:%M\")\n",
    "#                 except ValueError:\n",
    "#                     now = datetime.datetime.now()\n",
    "#                     dt_object = datetime.datetime.strptime(time_str + \" \" + str(now.year), \"%I:%M %p %b %d %Y\")\n",
    "#                     if dt_object > now:\n",
    "#                         dt_object = datetime.datetime.strptime(time_str + \" \" + str(now.year - 1), \"%I:%M %p %b %d %Y\")\n",
    "#                     return dt_object.strftime(\"%d-%m-%Y %H:%M\")\n",
    "\n",
    "#         except ValueError:\n",
    "#             logger.warning(f\"Invalid time string: {time_str}\")\n",
    "#             return None\n",
    "\n",
    "#     def __scrape_infinite_scroll(self):\n",
    "#         \"\"\"Scrapes data from a page with infinite scrolling.\"\"\"\n",
    "#         chrome_options = Options()\n",
    "#         chrome_options.add_argument(\"--headless\")\n",
    "#         chrome_options.add_argument(Scraper.CHROME_OPTIONS)\n",
    "#         driver = webdriver.Chrome(options=chrome_options)\n",
    "#         logger.info(\"Fetch the data from the website....\")\n",
    "#         try:\n",
    "#             driver.get(self.url)\n",
    "#             WebDriverWait(driver, Scraper.DRIVER).until(\n",
    "#                 EC.presence_of_element_located((By.CSS_SELECTOR, self.element_selector))\n",
    "#             )\n",
    "\n",
    "#             scroll_count = 0\n",
    "#             while scroll_count < Scraper.MAX_SCROLLS:\n",
    "#                 driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#                 time.sleep(Scraper.SCROLL_PAUSE_TIME)\n",
    "#                 try:\n",
    "#                     if not driver.find_elements(By.CSS_SELECTOR, \"/html\"):\n",
    "#                         logger.info(\"Reached end of content.\")\n",
    "#                         break\n",
    "#                 except:\n",
    "#                     pass\n",
    "#                 scroll_count += 1\n",
    "\n",
    "#             html = driver.page_source\n",
    "#             soup = BeautifulSoup(html, \"html.parser\")\n",
    "#             logger.info(\"The data fetched completed from the website....\")\n",
    "#             return soup\n",
    "\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"{e}\")\n",
    "#             return None\n",
    "#         finally:\n",
    "#             driver.quit()\n",
    "\n",
    "#     def get_the_data(self):\n",
    "#         ''' Get the data from the website'''\n",
    "#         return self.__scrape_infinite_scroll()\n",
    "\n",
    "#     def __find_Comments(self, stock_name):\n",
    "#         ''' Find All the comments in the page'''\n",
    "#         logger.info(\"Find all the comments in the page....\")\n",
    "#         elements = self.__scrape_infinite_scroll()\n",
    "#         comments = []\n",
    "#         all_comments = []\n",
    "\n",
    "#         for comment_element in elements.find_all(\"div\", class_=self.element_selector):\n",
    "#             comment_text = comment_element.get_text(strip=True)\n",
    "#             get_ts = comment_element.find_previous(\"div\", class_=self.timeStamp_selector)\n",
    "#             author = comment_element.find_previous(\"div\", class_=self.cmd_selector).text.strip()\n",
    "#             if get_ts:\n",
    "#                 timestamp_text = self.__convert_time_to_datetime(get_ts.find(\"div\").text.strip().replace(\"schedule\", \"\"))\n",
    "#             else:\n",
    "#                 timestamp_text = \"Timestamp not found\"\n",
    "\n",
    "#             all_comments.append({\"Stock\": stock_name, \"Comment\": comment_text, 'Commented By': author, \"Timestamp\": timestamp_text})\n",
    "\n",
    "#         logger.info(\"Successfully grab the Data!!\")\n",
    "#         return all_comments\n",
    "\n",
    "#     def __Convert_to_DataFrame(self, stock_name):\n",
    "#         ''' Convert the comments to a pandas dataframe'''\n",
    "#         all_comments = self.__find_Comments(stock_name)\n",
    "#         df = pd.DataFrame(all_comments)\n",
    "#         df = df.sort_values(by='Timestamp', ascending=True)\n",
    "#         return df\n",
    "\n",
    "#     def show_Comments(self, stock_name, text_area):\n",
    "#         ''' Display all the comments in the page'''\n",
    "#         data = self.__Convert_to_DataFrame(stock_name)\n",
    "#         logger.info(\"************* The DATA *************\")\n",
    "#         logger.info(data)\n",
    "#         text_area.delete(1.0, tk.END)\n",
    "#         text_area.insert(tk.END, data.to_string())\n",
    "\n",
    "#     def save_Comments(self, stock_name):\n",
    "#         ''' Save the comments to a csv file'''\n",
    "#         data = self.__Convert_to_DataFrame(stock_name)\n",
    "#         filepath = filedialog.asksaveasfilename(defaultextension=\".csv\", filetypes=[(\"CSV files\", \"*.csv\")])\n",
    "#         if filepath:\n",
    "#             data.to_csv(filepath, index=False)\n",
    "#             logger.info(f\"File has been generated successfully!!\")\n",
    "#             messagebox.showinfo(\"Success\", \"File saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
